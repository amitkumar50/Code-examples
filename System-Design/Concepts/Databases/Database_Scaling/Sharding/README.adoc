:toc:
:toclevels: 6

== link:https://youtu.be/jP6du10hFNs[Self Video]

== Partitioning/Sharding(MongoDB,ElasticSearch) / Region(HBase) / Bigtable(Tablet) / vNode(Cassandra) / vBucket(CouchBase)
- Replication(Multiple copies of the same data on different nodes). For very large datasets, breaking the data & storing into partitions is called partitioning or sharding.
- Breaking the Huge data into pieces and storing on different servers.
```c
hello world come here

|hello|     |world|     |come|        |here|
node-1      node-2      node-3        node-4
```
=== How Partioning achieves Scalabilty
- Different partitions can be placed on different nodes hence a large dataset is distributed across many disks, and the query load can be distributed across many machines/processors.
- With partitioning we speard query load evenly across nodes. That means 10 nodes should be able to handle 10 times read & write throughput wrt 1 node.

==== Partitioning with [Replication](/System-Design/Concepts/Databases/Database_Scaling/1.Replication)
- Keeping copies of each partition at multiple nodes improves fault tolerance.
- if master/slave replication is used with partitioning, Each node may be master for 1 dataset while slave for other
```c
  |             Node-1             |      |             Node-2             |
  |partition1 partition2 partition3|      |partition3 partition2 partition1|
  | (master)   (slave)    (slave)  |      | (master)   (slave)    (slave)  |
```

== Types of Sharding
=== 1. By Key range
Each partition/shard holds range of `<key, value>` for particular range. If we know which partition is assigned to which node, then we can make request directly to the appropriate node.
```c
       |Partition-1|   |Partition-2|   |Partition-3|
keys   |a-e        |   |f-o        |   |p-z        |
```
- *Adv:* Keys can be kept in sorted order inside the partition.
- *Disadv:*
  - _1. Certain types of keys can turn partition into <<hotspot, Hotspot>>. Eg: if a shard stores all tweets of a celebrity user

[[hashofkeys]]
=== 2. By Hash of Keys
```c
  key -> |Hash Function| -> Hash of Shard
```
Hash is so generated that keys are equally distributed amongst shards

image:Partitioning_by_hash_of_keys.PNG?raw=true[Partitioning_by_hash_of_keys]

* *Adv:*
** Similar keys different hashes are generated, hence hotspots are avoided.

* *Disadv:*
** _1. Range based key search property is lost._ ie Advantage of [Partitioning by Key range](#kr) is lost.
** - _2. Hotspots still exists:_ In extreme conditions, where keys differ by millisec, same hash gets generated and all load goes to same shard.

=== 3. link:/System-Design/Concepts/Databases/Indexing[By reverse/secondary indexes]

==== 3.1 Document Based / Per Node Indexing
- link:/System-Design/Concepts/Databases/Indexing[keyword to key mapping] is created.
- if someone want to search keyword, all partitions need to be queried.

==== 3.2 Term Based / Global Indexing
- Rather than storing indexes on every partition, We can construct a global index that covers data in all partitions.
- We should not store that index on one node, since it would likely become a bottleneck and defeat the purpose of partitioning.
- Example:
  - Colors from `a to r` from all partitions stored at partition=0.
  - Colors from `s to z` stored on partition 1.

image:Partitioning_by_term.PNG?raw=true[Partitioning_by_term]

- **Disadv:**
  - _1. Slower/complicated writes:_ Write to a single document(on 1 partition) may now affect multiple partitions of the index (every term in the document might be on a different partition, on a different node).

[[regionbasedpartition]]
=== 4. Region based Partition
* Storing data specific to region (maybe based on pin code).
* *Advantages:* 
** 1. Data Localization: Users in a particular region primarily access data relevant to that region, which can reduce latency and improve response times.
** 2. Scaling: By partitioning the database based on region, we can scale system horizontally.
** 3. Regulatory Compliance: Data privacy laws may require certain data to be stored within specific geographic regions
** 4. Disaster Recovery: In the event of a localized failure or disaster, having region-based partitions can make it easier to implement disaster recovery strategies. We can focus on recovering data for specific regions without affecting the entire system
** 5. Reduced Maintenance Downtime: When need to perform maintenance tasks, such as database upgrades, we can target specific regions, minimizing the impact on the overall system.

== Rebalancing Partitions
- if partition fails(as nodes do fail) then how to move data to other node? 
**Strategies for rebalancing**

=== 1. Fixed number of partitions
- Create more partitions on 1 node. In cluster of 10 nodes, create 1000 partitions. ie Every node contains 10 partitions.
- Whenever new node joins it takes few partitions from existing node(until partitions are fairly distributed once again).
<img src=rebalancing_fixed_no_of_partitions.PNG width=500/>

=== 2. Dynamic partitioning
> Eg: HBase, RethinkDB perform this.
- if partition size grows above threshold(HBase 10GB) it splits into 2 halves.
- Conversely, if lots of data is deleted and a partition shrinks below some threshold, it is merged with an adjacent partition.
- **Adv:** Adjusts to load. 
- **Disadv:** Until it hits the point at which the first partition is split, all writes processed to single node while the other nodes sit idle.

== Request Routing
How client/application server sends requests to a Parititon/Shard?

It Application server code responsibilty to keep knowledge of data present on shards, send query & Aggregate results.

Database frameworks(Eg: Diesel Rust or other) can create DB queries, open/close DB but complete knowledge of what data is on which shard should lie with Application server.

=== 1. Round Robin
- Allow clients to contact any node/partition (e.g. via a round-robin load balancer).
- If that node coincidentally owns the partition to which the request applies, it can handle the request directly; otherwise it forwards the request to the appropriate node.

=== 2. Routing tier first
- Send all requests from clients to a routing tier first, which determines the node that should handle the request and forwards it accordingly
- This tier is parition aware load balancer

=== 3. Direct Connect
Clients aware of partitioning and the assignment of partitions to nodes. Client can connect directly to the appropriate node, without any intermediary.

image:request_routing.PNG?raw=true[request_routing]

=== 4. Coordinator Service(Zookeeper)
- **Problem In Above 3 approaches:** How routing decision making component knows about changes in the assignment of partitions to nodes?
- Coordinator service(Zookeeper) will:
  - Keep track of cluster metadata(ie mapping of partitions to nodes)
  - Every node in cluster will register to Zookeeper.
- Examples: LinkedIn's Helix, HBase, SolrCloud and Kafka uses zookeeper.

image:zookeeper.PNG?raw=true[zoo]

== Terms
[[hotspot]]
=== Hotspot
Cluster has become highly [skewed](#sk), all load ends up on 1 partition. 9 out of 10 nodes are idle, and bottleneck is the 1 busy node. The partition with disproportion ately high load is called a hot spot.

=== Skewed
When partitioning becomes unfair some partitions have more data or queries than others. This makes the partitioning much less effective, this can lead to [hotspot](#hs)
